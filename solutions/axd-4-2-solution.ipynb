{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESE61003 - Audio Experience Design\n",
    "# Week 4 - Visualising Sound (using Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognising sound often involves finding a suitable time-frequency representation. This is both important for yourself, to visually inspect the sound and get an idea of its frequency contents, and for the recognition algorithm you use to automate recognition, to give it the most useful starting point. However, time-frequency representations come with a set of parameters that can drastically alter the visualisation, and careful trade-offs need to be made. In this session, you'll get some practical experience to gain insight into this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import ipywidgets as widgets\n",
    "from disiple.signals import AudioSignal, Spectrum, PowerSpectrum, Spectrogram\n",
    "from disiple.operations import apply_adsr, apply_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading some useful packages and creating a couple of recurring variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 4\n",
    "samplerate = 44100\n",
    "nyquist_freq = samplerate / 2\n",
    "time = np.arange(0, duration, 1/samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Frequency sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start properly, there is another type of basic signal that will come in useful later on when testing different visualisations. A _sine sweep_ or _chirp_ is a sinusoidal wave whose frequency changes steadily over time. You can create its samples using SciPy's [`signal.chirp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.chirp.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 1\n",
    "\n",
    "Create a couple of sine sweeps and listen to them. Try to create at least:\n",
    "\n",
    "- a sweep of increasing frequency\n",
    "- a sweep of decreasing frequency\n",
    "- a logarithmically increasing sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "increasing_sweep = AudioSignal(signal.chirp(t=time, f0=20, t1=duration, f1=nyquist_freq), samplerate)\n",
    "increasing_sweep.play()\n",
    "decreasing_sweep = AudioSignal(signal.chirp(t=time, f0=nyquist_freq, t1=duration, f1=20), samplerate)\n",
    "decreasing_sweep.play()\n",
    "log_increasing_sweep = AudioSignal(signal.chirp(t=time, f0=20, t1=duration, f1=nyquist_freq, method='logarithmic'), samplerate)\n",
    "log_increasing_sweep.play()\n",
    "log_decreasing_sweep = AudioSignal(signal.chirp(t=time, f0=nyquist_freq, t1=duration, f1=20, method='logarithmic'), samplerate)\n",
    "log_decreasing_sweep.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 2\n",
    "\n",
    "- What happens if either parameter `f0` or `f1` has a value that is higher than the Nyquist frequency?\n",
    "- What happens if parameter `t1` is set to a value that is different from the signal's duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "aliasing_sweep = AudioSignal(signal.chirp(t=time, f0=20, t1=duration, f1=samplerate), samplerate)\n",
    "aliasing_sweep.play()\n",
    "extrapolating_sweep = AudioSignal(signal.chirp(t=time, f0=20, t1=duration/4, f1=nyquist_freq/2, method='linear'), samplerate)\n",
    "extrapolating_sweep.play()\n",
    "log_extrapolating_sweep = AudioSignal(signal.chirp(t=time, f0=20, t1=duration/2, f1=nyquist_freq/4, method='logarithmic'), samplerate)\n",
    "log_extrapolating_sweep.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ Going past the Nyquist frequency causes aliasing, which mirrors frequencies back into the spectrum below the Nyquist frequency. For an increasing sweep, this will cause the frequencies to decrease and the other way around. If the parameter `t1` is not equal to the signal duration, the sweep will be extended beyond `t1` in the way defined by the `method` parameter. You can therefore create the exact same signals whether `t1=duration` or not, but the latter complicates precise control the result. It also increases the risk of accidentally passing the Nyquist frequeny, as illustrated by the examples. The logarithmic sweep even overshoots the Nyquist frequency so much that the aliasing becomes so strong that the result sounds noise-like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 3\n",
    "\n",
    "Plot the spectrum of a linear sweep from 0 to the Nyquist frequency. What does it remind you of and why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "Spectrum(increasing_sweep).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ The spectrum of a linear sweep is flat, in a way that is reminiscent of the ideal spectrum of white noise, but even more flat. The reason is that white noise by definition contains all frequencies in equal amount, but they are produced in a random temporal order. The signal is stochastic, therefore repeated instances will be slightly different and the longer the signal, the better it approximates its ideal flat spectrum. A linear sweep, conversely, is a deterministic signal that produces all frequencies in order, guaranteeing equal presence of all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Zooming into a frequency range of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're analysing a piece of real-world audio, we are often mostly interested in the lower frequency regions. That does not mean that the high frequency content is superfluous for playback, on the contrary, it gives sound its brightness and \"sparkle\" as you can hear by comparing the song below with its downsampled version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = AudioSignal('../data/Townhouse_Woods_-_Cat_Person.wav')\n",
    "resampled_song = song.resample(8000)\n",
    "song.play()\n",
    "resampled_song.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, most natural sounds decrease in power further up in the frequency spectrum, and many high frequencies are actually harmonics of lower frequencies. This means that there is not much information in the higher frequency regions that is not already more apparent in the lower frequency regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spectrum(song).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that reason, we often focus on a narrower range of interest when analysing the content of a sound signal. For instance, an upper limit of 4000 Hz means that only the fundamental frequency of the single highest [key on the piano](https://en.wikipedia.org/wiki/Piano_key_frequencies#List) ($C_8 = 4186$ Hz) cannot be represented, while most other instruments (including [soprano](https://en.wikipedia.org/wiki/Soprano) voice) produce far lower fundamental frequencies. This focussing on a particular range can be done literally by zooming into the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spectrum(song).display(x_range=(0, 4000), y_range=(-140, -35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, calculating a spectrum that includes the higher frequencies just for them to be ignored afterwards is waste of computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 4\n",
    "\n",
    "Can you think of a way to compute a spectrum with range 0-4000 Hz directly, without needing to throw away the high frequency bins? Try to demonstrate it by plotting the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ By downsampling the song to 8 kHz, the signal will contain only frequencies up to 4 kHz. If we calculate the Fourier transform of that downsampled signal, we will get a spectrum with frequencies between 0 and 4 kHz. You can visually compare the spectra above and below to verify that they are very similar.\n",
    "\n",
    "If you answered applying a low-pass filter to the signal, you're going in the right direction but are not completely there yet. Applying a low-pass filter with a cutoff frequency at 4 kHz will indeed remove (or at least reduce) all high-frequency content, but when we calculate its Fourier transform, the spectrum will still include frequencies above 4 kHz. This is what we did in the previous session, to show the result of low-pass filtering. The process of downsampling actually starts with a low-pass filtering stage to ensure that all frequencies above the new, lower Nyquist frequency are removed (which would cause aliasing in the downsampled version). It is then followed by a proportionate reduction of the number of samples such that the signal duration $d = \\dfrac{N}{f_s}$ remains constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "Spectrum(resampled_song).display(y_range=(-140, -35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency resolution $\\Delta f$ determines how precisely we can distinguish between neighbouring frequencies, which is important for applications like music transcription and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 5\n",
    "\n",
    "Work out what the effect of downsampling is on the frequency resolution. Remember that $\\Delta f = \\dfrac{f_s}{N}$ with $f_s$ the sample rate and $N$ the length of the signal. Illustrate your answer with plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ When we downsample a signal with samplerate $f_s$ and length $N$ (expressed in samples) by a factor $D$, the new sample rate will be $\\dfrac{f_s}{D}$ but also its length will decrease to $\\dfrac{N}{D}$ samples. The two divisions by $D$ cancel each other out, meaning that the frequency resolution remains unchanged after downsampling. Compare this with our encounter of $\\Delta f$ in the previous session, when we had the same number of random signals that we interpreted as different signals with increasing sample rates. In that case, $N$ remained constant but $f_s$ didn't, leading to increasing frequency resolutions (and decreasing durations).\n",
    "\n",
    "We can visualise the constant frequency resolution by plotting extreme close-ups of the spectra such that we can see individual data points. It is then easy to verify that the distance between the discrete frequencies is the same for the original and the resampled signal.\n",
    "\n",
    "Good news then, we can use downsampling followed by a Fourier transform as an alternative to taking the Fourier transform of the original signal and ignoring the highest frequencies. Whether this leads to actual savings in computational power depends on whether the difference in cost between the two Fourier transforms is high enough to compensate for the additional cost of the resampling algorithm. There are also other reasons that determine if it makes sense to use downsampled signals or not, such as storage and transfer costs of larger versus smaller files or whether a signal will need to be reconstructed from its spectrum later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "delta_freq = song.samplerate / len(song)\n",
    "start_freq = 1000\n",
    "freq_range = (start_freq, start_freq + 30*delta_freq)\n",
    "dB_range = (-95, -75)\n",
    "Spectrum(song).display(x_range=freq_range, y_range=dB_range, title='Spectrum close-up of original song')\n",
    "Spectrum(resampled_song).display(x_range=freq_range, y_range=dB_range, title='Spectrum close-up of resampled song')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The power of a signal and its spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the spectrum of a signal is highly informative. One of the reasons is that the Fourier transform preserves power, a property that is known as [Parseval's theorem](https://en.wikipedia.org/wiki/Parseval%27s_theorem). Given a signal $x(n)$ of length $N$, we can calculate its (average) power $P_x$ as $$P_x = \\dfrac{1}{N}\\sum_{n=0}^{N-1} |x(n)|^2$$\n",
    "\n",
    "We can get the power of any `AudioSignal` by calling the method `.power()`, which is by default expressed in decibels, but can be returned as a linear value by passing the `dB=False` argument. For instance, the power of a sine wave is -3 dB or 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 220\n",
    "sine_signal = AudioSignal(np.sin(2*np.pi*freq*time), samplerate)\n",
    "sine_signal.power(), sine_signal.power(dB=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 6\n",
    "\n",
    "What type of signal has maximal power and what is its value? Verify with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "square_signal = AudioSignal(signal.square(2*np.pi*freq*time), samplerate)\n",
    "max_dc_signal = AudioSignal(np.ones_like(time), samplerate)\n",
    "square_signal.power(), max_dc_signal.power()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ Because the maximal amplitude in a digital signal is 1, a signal will have maximal power when the absolute value of its amplitude is always 1. Any square wave or signal consisting of constant 1s or -1s matches this description. They have a power of 0 dB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a similar method `.power()` on variables of type `Spectrum`, and verify that the Fourier transform indeed preserves power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_spectrum = Spectrum(sine_signal)\n",
    "sine_spectrum.power(), sine_spectrum.power(dB=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a spectrum so informative, is that we don't just get the power of the whole signal, but the power of individual frequency components. For this, we simply need to square the magnitudes of the (complex) spectrum. When we are expressing spectral values in dB, this is already included in the calculation (because decibels are only defined for power ratios), but if we want to calculate linear values of the so-called _power spectrum_ with the `disiple` library, we can either pass the argument `exponent=2` to `Spectrum` or directly create a variable of type `PowerSpectrum` (both do exactly the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_power_spectrum = Spectrum(sine_signal, dB=False, exponent=2)\n",
    "# OR\n",
    "sine_power_spectrum = PowerSpectrum(sine_signal, dB=False)\n",
    "sine_power_spectrum.display(x_range=(200, 240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 7\n",
    "\n",
    "What strange phenomenon can you notice in the plot? Could you think of an explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ Even though the Fourier tranform is supposed to preserver power, the power at the frequency of the sine is only 0.25, whereas it should be 0.5 according to our previous calculation. Where did the other half of the signal power go? The thing to remember about the Fourier transform is that it decomposes a signal into frequency components between $-f_\\mathrm{Nyq}$ and $f_\\mathrm{Nyq}$. We tend to ignore the half with negative frequencies for real signals such as audio, because we know it to be a mirrored copy of the positive half of the spectrum, but that also means that half of the power ends up in the negative half of the spectrum. This unseen half is accounted for in the calculation of the power of the whole signal though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although mathematically correct, showing only half the signal's power in a plot of the spectrum can be counter-intuitive. Therefore it's possible to add an additional argument `norm_single_side_band=True` to `Spectrum` or `PowerSpectrum` that performs a rescaling such that the total power per frequency component can be easily read in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PowerSpectrum(sine_signal, dB=False, norm_single_side_band=True).display(\n",
    "    x_range=(200, 240), y_range=(0, 0.55),\n",
    "    title='Spectrum of a sine wave normalised to show all power in the positive frequencies',\n",
    ")\n",
    "PowerSpectrum(sine_signal, dB=True, norm_single_side_band=True).display(\n",
    "    x_range=(200, 240), y_range=(-20, 0),\n",
    "    title='Spectrum of a sine wave normalised to show all power in the positive frequencies',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this option, it is immediately clear that the power of our sine wave is 0.5 or 3 dB, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing this distribution of power over different frequencies becomes more useful whent plotting more complex signals, such as a sequence of two sines of different frequencies (which is created by concatenating multiple `AudioSignal`s with the `&` operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(ratio=widgets.IntSlider(min=1, max=10, step=1, value=2, description='Ratio of sine durations',\n",
    "                                           style={'description_width': 'initial'}, layout=widgets.Layout(width='50%')))\n",
    "def sequential_signals(ratio):\n",
    "    sine2_length = round(len(sine_signal)/ratio)\n",
    "    sine2_signal = AudioSignal(np.sin(2*np.pi*2*freq*time[:sine2_length]), samplerate)\n",
    "    seq_signal = sine_signal & sine2_signal\n",
    "    seq_signal.play()\n",
    "    seq_spectrum = PowerSpectrum(seq_signal, norm_single_side_band=True)\n",
    "    seq_spectrum.display(\n",
    "        title='Spectrum of two sines in sequence with a duration ratio of {}:1'.format(ratio),\n",
    "        x_range=(freq-50, 2*freq+50), y_range=(-25, 0)\n",
    "    )\n",
    "    print(f'The average power in the whole signal is {seq_signal.power():.1f} dB')\n",
    "    delta_freq = seq_signal.samplerate / len(seq_signal)\n",
    "    freq_idx = round(freq / delta_freq)\n",
    "    two_freq_idx = round(2 * freq / delta_freq)\n",
    "    print(f'The power at {freq} Hz is {seq_spectrum.magnitude[freq_idx]:.1f} dB and the power at {2*freq} Hz is {seq_spectrum.magnitude[two_freq_idx]:.1f} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 8\n",
    "\n",
    "Play around with the interactive example above which contains a first sine of fixed duration, followed by a second sine of double the frequency and a duration that's controlled by the slider. The slider set to 2 will make the second sine half the length of the first, and so on. Observe how the power of both sine components in the spectrum fluctuates with the duration ratio. What is the relationship between the duration ratio and the power ratio of the two sines? Why does the power of the first sine change although its duration is fixed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ The power ratio between the two sines has a quadratic dependence on the ratio of durations. For a ratio of duration $r$, the difference in power between the two sines is $10*\\log_{10}(r^2)$ decibels. Even though the first sine has constant duration, its proportion in the overall signal changes depending on the varying length of the second sine. Therefore the power of the first sinusoidal component in the spectrum changes, as it is averaged over the whole signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 9\n",
    "\n",
    "Because a spectrum is calculated for the whole length of a signal and therefore discards all temporal information, there are multiple different signals that have the same spectrum. Can you create another signal that has the same spectrum as the sequence of two sines above? Don't bother making your answer interactive, just pick a single value for the power ratio as long as it's not 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ Because the spectrum is only influenced by the ratio of the duration of the two sines, not by their order, the simplest way to create a distinct signal with the same spectrum would be to swap the order of the sines. Since that's almost too easy, you might consider using the fact that only the ratio of the durations is important, not the absolute duration. You could for instance reduce the duration of both sines proportionally by a factor $n$. That gives you roughly the same spectrum, but here the frequency resolution pops up again. Since the overall signal is now $n$ times shorter, the resolution of its spectrum has increased by $n$. To decrease the resolution again to its original level, we can in this specific case repeat our shorter signal $n$ times. What's so special about this case we will discuss in the next section. Finally you could consider playing the two sines together instead of in sequence. You'd then vary the ratio of their amplitudes in order to get the desired power ratio. Although theoretically possible, it becomes analytically quite complicated to derive the exact amplitude ratio for the finite-time signals we're creating. The reason for this complexity is the same special situation (or lack of to be precise in this case) that explains why we can decrease the frequency resolution simply by repeating the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "shorter_sine1_length = round(len(sine_signal)/2)\n",
    "# divided by two here but could be any positive integer n as long as you use n copies to get to the original signal duration\n",
    "shorter_sine1_signal = AudioSignal(np.sin(2*np.pi*freq*time[:shorter_sine1_length]), samplerate)\n",
    "\n",
    "@widgets.interact(ratio=widgets.IntSlider(min=1, max=10, step=1, value=2, description='Ratio of sine durations',\n",
    "                                           style={'description_width': 'initial'}, layout=widgets.Layout(width='50%')))\n",
    "def shorter_sequential_signals(ratio):\n",
    "    shorter_sine2_length = round(shorter_sine1_length/ratio)\n",
    "    shorter_sine2_signal = AudioSignal(np.sin(2*np.pi*2*freq*time[:shorter_sine2_length]), samplerate)\n",
    "    shorter_seq_signal = shorter_sine1_signal & shorter_sine2_signal & shorter_sine1_signal & shorter_sine2_signal\n",
    "    shorter_seq_signal.play()\n",
    "    shorter_seq_spectrum = PowerSpectrum(shorter_seq_signal, norm_single_side_band=True)\n",
    "    shorter_seq_spectrum.display(\n",
    "        title='Spectrum of two sines in sequence with a duration ratio of {}:1 repeated twice'.format(ratio),\n",
    "        x_range=(freq-50, 2*freq+50), y_range=(-25, 0)\n",
    "    )\n",
    "    print(f'The average power in the whole signal is {shorter_seq_signal.power():.1f} dB')\n",
    "    delta_freq = shorter_seq_signal.samplerate / len(shorter_seq_signal)\n",
    "    freq_idx = round(freq / delta_freq)\n",
    "    two_freq_idx = round(2 * freq / delta_freq)\n",
    "    print(f'The power at {freq} Hz is {shorter_seq_spectrum.magnitude[freq_idx]:.1f} dB and the power at {2*freq} Hz is {shorter_seq_spectrum.magnitude[two_freq_idx]:.1f} dB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The difference between best-case and real-world scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all examples given so far, the exact frequency and power of the sine signal has been easy to read from the spectrum. Unfortunately, that was because these sine signals were all special cases. Remember that a spectrum computed from a digital waveform is a discrete signal itself, which is defined for specific frequencies only, namely integer multiples of $\\Delta f$. The sine waves we've created so far all happened to have a frequency that matched one of the points of this discrete frequency axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq in sine_spectrum.frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the spectrum look like then for a sine with a frequency that falls between these discrete points? As you can imagine, its power is spread over surrounding frequency points or _bins_. Not just the directly adjacent ones, although those are most affected, but also some that are further away. This phenomenon is known as _spectral leakage_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 220*2**(7/12)\n",
    "sine_signal = AudioSignal(np.sin(2*np.pi*freq*time), samplerate)\n",
    "sine_signal.power(), sine_signal.power(dB=False)\n",
    "sine_power_spectrum = PowerSpectrum(sine_signal, dB=False, norm_single_side_band=True)\n",
    "sine_power_spectrum.display(\n",
    "    x_range=(freq-20, freq+20), y_range=(0, 0.25),\n",
    "    title='Spectrum of a sine wave with a frequency that is not an exact multiple of Δf',\n",
    ")\n",
    "print(f'The power of the sine is {sine_power_spectrum.power(dB=False):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes therefore harder to verify both what the exact frequency and exact power of the sine wave are. Note that the total power in the spectrum is the same whether the sine frequency falls on a bin frequency or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the leaking of the sinusoidal peak into the surrounding bins is still relatively predictable for isolated sines (and interpolation strategies exist to reconstruct the exact peak), the view becomes more muddled when multiple frequencies are involved. Especially since we also have the spectral resolution to deal with, as will become apparent in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 220\n",
    "delta_freq = samplerate / len(time)\n",
    "base_sine = AudioSignal(0.5 * np.sin(2*np.pi*freq*time), samplerate)\n",
    "close_sine = AudioSignal(0.5 * np.sin(2*np.pi*(freq+1.5*delta_freq)*time), samplerate)\n",
    "too_close_sine = AudioSignal(0.5 * np.sin(2*np.pi*(freq+0.5*delta_freq)*time), samplerate)\n",
    "indistinguishable_mix = base_sine + too_close_sine\n",
    "distinguishable_mix = base_sine + close_sine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the snippet above, we've created three sine waves with close frequencies and combine them into two mixtures. The first mixture contains two sines whose frequency difference is larger than $\\Delta f$, whereas the second mixture contains two sines with a frequency difference below $\\Delta f$. Although the difference between the individual sines is barely noticeable, their combinations are notably different (a temporal evolution of their amplitudes is visible due to the two sines periodically cancelling each other out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sine.play()\n",
    "base_sine.display(title='Single sine wave')\n",
    "indistinguishable_mix.play()\n",
    "indistinguishable_mix.display(title='Combination of two sines with frequency separation below Δf')\n",
    "distinguishable_mix.play()\n",
    "distinguishable_mix.display(title='Combination of two sines with frequency separation above Δf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the spectra of the mixed signals, it is hard to decide how many sines are present in the signal. As you'd expect for the two sines that are closer to each other than $\\Delta f$, they are fused together into one wide peak, but similar wide peaks appear for single sines whose frequency is not a multiple of $\\Delta f$. What's worse is that even the mixture of two sines with frequencies separated by more than $\\Delta f$ does not give two clear peaks. Because the second peak falls between discrete frequency points, part of it gets distributed to the first peak, again creating a single fused peak. In practice it can therefore be very hard to distinguish between two audibly distinct signals based solely on their spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spectrum(base_sine, dB=False).display(x_range=(freq-2, freq+2), y_range=(0, 0.3), title='Spectrum of a single sine wave')\n",
    "Spectrum(indistinguishable_mix, dB=False).display(\n",
    "    x_range=(freq-2, freq+2), y_range=(0, 0.3), \n",
    "    title='Spectrum of a combination of two sines with frequency separation below Δf',\n",
    ")\n",
    "Spectrum(distinguishable_mix, dB=False).display(\n",
    "    x_range=(freq-2, freq+2), y_range=(0, 0.3), \n",
    "    title='Spectrum of a combination of two sines with frequency separation above Δf',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 10\n",
    "\n",
    "Rerun the example above for a number of sines with frequencies that both coincide or not with the discrete frequency bins. Can you find frequencies for which two separate peaks are clearly visible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ A frequency of $220*2^\\frac{1}{12}$ for the first signal leads to two clearly identifiable peaks in the middle spectrum, whereas a frequency of $220*2^\\frac{1}{12}$ merges  the two peaks even more. There's no easy analytical expression to predict which frequency will lead to what behaviour (apart from calculating the Fourier transform itself), you just need to randomly explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualising time-varying signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can only derive the power of the frequencies in the _entire_ signal from a spectrum, without possibility to distinguish between power inequalities caused by amplitude or duration differences, it is most useful for signals that don't vary over time (or if we don't care about temporal variations). In case we do want to observe the variation over time, we can use another signal representation, the _spectrogram_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the idea of a spectrogram is relatively simple. You divide the signal in multiple shorter segments and calculate a spectrum for each of them. It does however introduce additional parameters that can significanty alter the visualisation and therefore need careful consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first relevant parameter is the width of each segment. Since we call these segments _frames_, the width is also know as the _frame width_, which we express in samples. We create a spectrogram with the `disiple` library by passing a signal to a `Spectrogram` constructor and set the option `frame_size` to an integer, 2048 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_down_sweep = AudioSignal(signal.chirp(t=time, f0=20, t1=duration, f1=samplerate), samplerate)\n",
    "up_down_sweep.play()\n",
    "specgram = Spectrogram(up_down_sweep, frame_size=2048)\n",
    "specgram.display(title='Spectrogram with a frame size of 2048')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting picture, we can now clearly see the temporal evolution of the sine sweep, first going up and then coming down again (created by intentionally using aliasing). Each of the columns in the picture is a spectrum of a frame, which we can show individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_at_1s = specgram.spectrum_at(time=1)\n",
    "spectrum_at_1s.display(title='Spectrum of the frame at 1 s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 11\n",
    "\n",
    "Since each column of the spectrogram is simply a spectrum, all the real-world challenges we've seen so far apply to spectrograms as well. An extra complication is that in practice, the frame size will be short compared to the length of a typical signal. This makes the frequency resolution $\\Delta f$ become significant for yet another reason. Can you think of why that is? Hint: it imposes another condition on the signals we can represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ Because the frequency resolution is also the frequency of the second bin (after the first which is always 0 Hz), it provides a lower limit to the frequencies that can be present in a signal. Any lower frequency would have a period that is longer than the signal length. The signals we've analysed so far all had a duration $d$ in the order of seconds, and since $\\Delta f = \\dfrac{1}{d}$ that meant the lowest representable frequency was far below the auditory threshold. For short fragments, this lower limit shifts upwards into the hearing range. For instance, for $f_s=44100\\,\\mathrm{Hz}$ a common frame size of 2048 gives $f_{min}=\\Delta f=21.5\\,\\mathrm{Hz}$ and a frame size of 1024 $f_{min}=43\\,\\mathrm{Hz}$. With these frequency resolutions, it becomes impossible to distinguish between the lowest [notes of a piano](https://en.wikipedia.org/wiki/Piano_key_frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand, we'd like to make our frames wide, since it improves the frequency resolution. On the other hand, a wide frame has the same drawback as a spectrum: we don't know when exactly during the frame the computed frequencies are present. Therefore a fine frequency resolution leads to a coarse _time resolution_. Even though we'd like them to be as fine as possible, we cannot have both since they are inversely related, which is a manifestation of [Heisenberg's uncertainty principle](https://en.wikipedia.org/wiki/Uncertainty_principle#Signal_processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrowband_specgram = Spectrogram(up_down_sweep, frame_size=8192)\n",
    "narrowband_specgram.display(title='Spectrogram with a frame size of 8192')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the right frame size for a particular signal and application is therefore always a trade-off. For instance, the spectrogram above is calculated with wider frames of 8192 samples (leading to a smaller $\\Delta f$ and narrower frequency bands). However, because the signal is a frequency sweep, it varies strongly over time (known as being _non-stationary_). Therefore the wider frame contains more different frequencies, which all get blended together. Together with the frequencies leaking into surrounding bins, this actually makes the precise frequency determination harder, despite the increased frequency resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, to make control over the frequency and time resolution more explicit regardless of sample rate, it is also possible to specify the frame width as a duration in seconds with the `frame_duration` option, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenHz_specgram = Spectrogram(up_down_sweep, frame_duration=0.100)\n",
    "tenHz_specgram.display(title='Spectrogram with a frame duration of 100 ms (therefore frequency resolution of 10 Hz)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively independent of the frame width, we also need to specify the distance between two consecutive frames. Similarly to the frame width, we have the choice of passing this distance as an option to the `Spectrogram` constructor either as a size in samples with `step_size` or as a duration in seconds with `step_duration`.\n",
    "\n",
    "Setting `step_size` equal to the `frame_size` will create contiguous, non-overlapping segments, which can be a good idea if your signal consists of regular events and you don't want to analyse across their boundaries, e.g. you want to slice a piece of music along its beats (assuming that you know the tempo and know it to be very regular). Having a `step_size` larger than the `frame_size` will leave gaps in the signal that are not included in any frame, so that's rarely a good idea. In most cases though, a signal will consist of temporal events that are irregular or have an unknown tempo.\n",
    "\n",
    "Suppose that you have a signal that contains some event that can occur at any time and that you want to recognise. Examples of such events include music notes or words. Even if you've set the frame width large enough to contain the whole event, using non-overlapping segments does not ensure that the whole event is captured in a single frame. It could be that the event only starts halfway a frame, with its latter half ending up in the next frame, but any possible split of the event between frames is possible. If you rely on having the whole event in a single frame for its recognition, for instance for recognising a keyword, this will lead to errors.\n",
    "\n",
    "The solution is to let frames overlap. The most extreme case is to advance only a single sample between frames, which is known as a _sliding window_. Doing so means that there always will be a frame that aligns with the start of your event. Note that in this case there will still be many other frames that contain (parts of) the event starting at a later point in the frame, which might confuse your recognition or lead to recognising multiple instances. In other situations, you don't need the entire event in a single frame to be recognised. For instance, you can detect the presence of a musical note based on only a fragment. In that case, you can use larger step size, but still might want to have some overlap to ensure that there's at least a frame containing the majority of the event.\n",
    "\n",
    "A good default is to use a shift size that is half the frame size, which is what's used when no step is specified in `Spectrogram`. When applying a windowing function to each frame, which is commonly done to reduce spectral leakage, a step of maximum half the frame size is necessary such that each sample appears near the middle of at least one frame, where the sample value remains mostly unchanged.\n",
    "\n",
    "Another way to look at the step size is that it determines the sample rate of the spectrogram signal. You can consider a spectrogram as a new time signal, which is no longer one-dimensional with a single amplitude per point in time, but multi-dimensional giving values for a fixed number of bins at each time point. Having a step size of 1 will cause the rate of the spectrogram to be equal to the audio signal's sample rate, any higher step size will make the spectrogram have a lower rate. This reduction of sample rate can be welcome, as it reduces the amount of data that needs to be processed or stored. For instance, suppose we want to use a spectrogram to determine what music notes are being played. It is probably excessive to localise the exact start of a note to the microsecond (which is the temporal resolution you get for common audio sample rates), as we don't perceive such small differences anyway. Instead we can work with spectrograms calculated every millisecond and save some computational effort by choosing a step size in the order of 100.\n",
    "\n",
    "\n",
    "In conclusion, although the step between frames can theoretically be changed independently of the frame width, in practice it needs to be tuned together with the frame width to a particular type of signal and application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 12\n",
    "\n",
    "Play around with the `step_size` or `step_duration` options of `Spectrogram` and see how it changes the spectrogram. Note that drawing spectrograms with a large number of points can take some time. A good part of the time is spent creating the tooltips, which you can disable by passing `tooltips=None` to the `.display()` method in order to speed up drawing. Nonetheless, very detailled spectrograms with wide frames and small steps will likely cause your browser to run out of memory. So decrease `step_size` in small amounts instead of immediately setting it to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "step_specgram = Spectrogram(up_down_sweep, frame_size=8192, step_size=1024)\n",
    "step_specgram.display(title='Spectrogram with a frame size of 8192 and a step size of 1024 samples', tooltips=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. The spacing of frequency bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final option in adapting the visualisation of a signal to a specific purpose is the distribution of frequency bins. The Fourier transform provides frequencies that are linearly spaced on the discrete frequency axis. However, that does not match the perception of our hearing, nor the [harmonic series](https://en.wikipedia.org/wiki/Harmonic_series_(music)). Therefore it  makes sense for some signals to display frequencies on a logarithmic axis.\n",
    "\n",
    "We achieve this by passing the option `spacing='log'` to the `Spectrogram` constructor. Because a logarithmic axis cannot start with a frequency of 0 Hz (which would map to $-\\infty$), we need to explicitly specify a minimum frequency as `min_freq`. This minimum frequency needs to be high enough to fit into the given frame width, and will throw an error if this is not the case. Alternatively, you can let the minimum frequency determine the frame width, by making the latter just wide enough to fit a single period of this frequency. Then you don't need to pass the frame width in any other way. Beware though, when used in this way, the minimum frequency also determines the frequency resolution, so make sure to keep it low.\n",
    "\n",
    "You can optionally set a maximum frequency with `freq_max` to zoom in on a specific frequency range. By default it is set to the Nyquist frequency. Finally, you also need to specify the number of bins per semitone with `num_bins`. A value of 1 calculates frequency bins that coincide with music notes, a value of 2 places an extra bin between each music note and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_specgram = Spectrogram(up_down_sweep, spacing='log', min_freq=27.5, num_bins=1)\n",
    "log_specgram.display(title='Logarithmically spaced spectrogram of a linearly increasing sine sweep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomed_log_specgram = Spectrogram(up_down_sweep, frame_duration=1/20, spacing='log', min_freq=1760, max_freq=3520, num_bins=3)\n",
    "zoomed_log_specgram.display(title='Logarithmically spaced spectrogram with frequencies between 1760 and 3520 Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we'd be only interested in _displaying_ frequencies on a logarithmic axis, it would suffice to use standard plotting functionality to interpolate linear spectrogram pixels onto a logarithmic axis. Instead we want to calculate the actual logarithmic spectrum values such that they can be used for further processing (and display the corresponding pixel values on an equidistant grid). For this implementation, a filterbank that redistributes the power of the linear bins over the appropriate logarithmic bins is used. This two-stage process is simple, but also means that reconstruction of the signal from a logarithmic spectrogram is impossible, whereas it is possible for a linear spectrogram (under certain constraints for its parameters). Direct computations of a logarithmic spectrogram exist, which are invertible, but they are computationally more intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 13\n",
    "\n",
    "Create a signal of multiple harmonic signals with different fundamental frequencies in sequence. Plot spectrograms with both linear and logarithmic frequency axes and appropriate parameters. Zoom in on the harmonic frequency patterns. Wat do you notice about way these patterns are visualised in the different spectrograms and why might one be preferred over the other? You might want to create a function that creates a harmonic signal for a given frequency to avoid duplicating code. Or if you're in a hurry, you can use a triangular wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "strip"
    ]
   },
   "outputs": [],
   "source": [
    "# Easy version with triangular waves\n",
    "def harmonic_signal(freq, time, samplerate):\n",
    "    return AudioSignal(signal.sawtooth(2*np.pi*freq*time, width=0.5), samplerate)\n",
    "\n",
    "freq = 220\n",
    "seq_signal = harmonic_signal(freq, time, samplerate) & harmonic_signal(freq*2**(7/12), time, samplerate) & harmonic_signal(2*freq, time, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "strip"
    ]
   },
   "outputs": [],
   "source": [
    "# Proper version with stacked sine waves\n",
    "def harmonic_signal(freq, time, samplerate, num_harmonics=10):\n",
    "    harmonic_samples = np.sum([1/num_harmonics * np.sin(2*np.pi*h*freq*time) for h in range(num_harmonics)], axis=0)\n",
    "    return AudioSignal(harmonic_samples, samplerate)\n",
    "\n",
    "freq = 220\n",
    "seq_signal = harmonic_signal(freq, time, samplerate) & harmonic_signal(freq*2**(7/12), time, samplerate) & harmonic_signal(2*freq, time, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "Spectrogram(seq_signal, frame_size=2048).display(\n",
    "    y_range=(55,4000),\n",
    "    title='Linear spectrogram between 55 and 4000 Hz of three harmonic signals',\n",
    ")\n",
    "Spectrogram(seq_signal, frame_size=2048, spacing='log', min_freq=55, max_freq=4000, num_bins=1).display(\n",
    "    title='Logarithmic spectrogram between 55 and 4000 Hz of three harmonic signals',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "_Answer:_ In the linear spectrogram, the distance between harmonics varies depending on the fundamental frequency, whereas this distance is constant in the logarithmic spectrogram. The fact that harmonics have a consistent pattern in a logarithmic spectrogram, which only shifts depending on the fundamental frequency, makes it easier to automatically recognise the presence of a harmonic signal with some form of pattern recognition algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
